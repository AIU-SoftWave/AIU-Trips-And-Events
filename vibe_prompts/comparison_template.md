# Platform Comparison Report

> Use this template to compare multiple AI platforms on the same prompt

---

# Comparison Report: [PROMPT_ID] - [Test Name]

## Overview

| Field | Value |
|-------|-------|
| **Prompt ID** | [e.g., P001] |
| **Test Name** | [e.g., Observer Pattern for Notifications] |
| **Platforms Tested** | [List all platforms] |
| **Test Date** | [YYYY-MM-DD] |
| **Tester** | [Your Name] |

**Test Objective:** [What you're comparing]

---

## Platforms Tested

### Platform 1: [Name]
- **Model:** [Version]
- **Report:** `vibe_prompts/reports/PROMPT_ID_platform1_report.md`

### Platform 2: [Name]
- **Model:** [Version]
- **Report:** `vibe_prompts/reports/PROMPT_ID_platform2_report.md`

### Platform 3: [Name]
- **Model:** [Version]
- **Report:** `vibe_prompts/reports/PROMPT_ID_platform3_report.md`

[Add more platforms as needed]

---

## Summary Comparison Table

| Metric | Platform 1 | Platform 2 | Platform 3 | Winner |
|--------|-----------|-----------|-----------|--------|
| **Generation Time** | [X min] | [Y min] | [Z min] | |
| **Lines of Code** | [X] | [Y] | [Z] | |
| **Files Generated** | [X] | [Y] | [Z] | |
| **Compilation Success** | [X%] | [Y%] | [Z%] | |
| **Quality Score** | [X/10] | [Y/10] | [Z/10] | |
| **Pattern Correctness** | [X%] | [Y%] | [Z%] | |
| **SOLID Adherence** | [X%] | [Y%] | [Z%] | |
| **Documentation** | [X/10] | [Y/10] | [Z/10] | |
| **Test Coverage** | [X%] | [Y%] | [Z%] | |
| **Manual Fixes** | [X] | [Y] | [Z] | |

---

## Detailed Category Comparison

### Speed Analysis

**Winner:** [Platform Name]

| Platform | Time (min) | Speed Rating |
|----------|-----------|--------------|
| Platform 1 | [X] | [Fast/Normal/Slow] |
| Platform 2 | [Y] | [Fast/Normal/Slow] |
| Platform 3 | [Z] | [Fast/Normal/Slow] |

**Analysis:**
- Fastest: [Platform] at [X minutes]
- Slowest: [Platform] at [Y minutes]
- Average: [Z minutes]

**Comments:** [Your observations]

---

### Quality Analysis

**Winner:** [Platform Name]

| Platform | Overall Score | Quality Rating |
|----------|--------------|----------------|
| Platform 1 | [X/10] | [Excellent/Good/Fair] |
| Platform 2 | [Y/10] | [Excellent/Good/Fair] |
| Platform 3 | [Z/10] | [Excellent/Good/Fair] |

**Breakdown by Category:**

| Category | Platform 1 | Platform 2 | Platform 3 |
|----------|-----------|-----------|-----------|
| Compilation | [X/10] | [Y/10] | [Z/10] |
| Pattern Implementation | [X/10] | [Y/10] | [Z/10] |
| Architecture | [X/10] | [Y/10] | [Z/10] |
| Documentation | [X/10] | [Y/10] | [Z/10] |
| Testing | [X/10] | [Y/10] | [Z/10] |
| Best Practices | [X/10] | [Y/10] | [Z/10] |

**Comments:** [Your observations]

---

### Compilation Success

**Winner:** [Platform Name]

| Platform | Compilation | Errors | Warnings |
|----------|------------|--------|----------|
| Platform 1 | [X%] | [N] | [N] |
| Platform 2 | [Y%] | [N] | [N] |
| Platform 3 | [Z%] | [N] | [N] |

**Common Issues:**
- [Issue 1]
- [Issue 2]

**Comments:** [Your observations]

---

### Pattern Implementation

**Winner:** [Platform Name]

| Platform | Pattern Correctness | Implementation Quality |
|----------|-------------------|----------------------|
| Platform 1 | [X%] | [Score/10] |
| Platform 2 | [Y%] | [Score/10] |
| Platform 3 | [Z%] | [Score/10] |

**Analysis:**
- Which platform best understood the pattern?
- Which had the cleanest implementation?
- Which had the most practical approach?

**Comments:** [Your observations]

---

### Code Organization

**Winner:** [Platform Name]

| Platform | Structure Quality | SOLID Score | Maintainability |
|----------|------------------|-------------|-----------------|
| Platform 1 | [X/10] | [X%] | [High/Med/Low] |
| Platform 2 | [Y/10] | [Y%] | [High/Med/Low] |
| Platform 3 | [Z/10] | [Z%] | [High/Med/Low] |

**Comments:** [Your observations]

---

### Documentation Quality

**Winner:** [Platform Name]

| Platform | Docs Score | JavaDoc | Comments | Examples |
|----------|-----------|---------|----------|----------|
| Platform 1 | [X/10] | [Yes/No] | [Good/Fair/Poor] | [Yes/No] |
| Platform 2 | [Y/10] | [Yes/No] | [Good/Fair/Poor] | [Yes/No] |
| Platform 3 | [Z/10] | [Yes/No] | [Good/Fair/Poor] | [Yes/No] |

**Comments:** [Your observations]

---

### Error Handling

**Winner:** [Platform Name]

| Platform | Error Handling | Exception Types | Recovery |
|----------|---------------|----------------|----------|
| Platform 1 | [Good/Fair/Poor] | [Comprehensive/Basic/None] | [Yes/No] |
| Platform 2 | [Good/Fair/Poor] | [Comprehensive/Basic/None] | [Yes/No] |
| Platform 3 | [Good/Fair/Poor] | [Comprehensive/Basic/None] | [Yes/No] |

**Comments:** [Your observations]

---

## Strengths & Weaknesses

### Platform 1: [Name]

**✅ Strengths:**
1. [Strength 1]
2. [Strength 2]
3. [Strength 3]

**❌ Weaknesses:**
1. [Weakness 1]
2. [Weakness 2]
3. [Weakness 3]

**Best For:** [Scenarios where this platform excels]

---

### Platform 2: [Name]

**✅ Strengths:**
1. [Strength 1]
2. [Strength 2]
3. [Strength 3]

**❌ Weaknesses:**
1. [Weakness 1]
2. [Weakness 2]
3. [Weakness 3]

**Best For:** [Scenarios where this platform excels]

---

### Platform 3: [Name]

**✅ Strengths:**
1. [Strength 1]
2. [Strength 2]
3. [Strength 3]

**❌ Weaknesses:**
1. [Weakness 1]
2. [Weakness 2]
3. [Weakness 3]

**Best For:** [Scenarios where this platform excels]

---

## Key Findings

### Pattern Recognition
[Which platforms handled the pattern best and why?]

### Code Structure
[Which produced the best architecture and organization?]

### Completeness
[Which generated the most complete solution?]

### Practicality
[Which solution would be easiest to use in production?]

### Innovation
[Did any platform show creative or unexpected approaches?]

---

## Recommendations

### For Backend Development
**Best Choice:** [Platform]  
**Reason:** [Explanation]

### For Frontend Development
**Best Choice:** [Platform]  
**Reason:** [Explanation]

### For Pattern Implementation
**Best Choice:** [Platform]  
**Reason:** [Explanation]

### For Rapid Prototyping
**Best Choice:** [Platform]  
**Reason:** [Explanation]

### For Production Code
**Best Choice:** [Platform]  
**Reason:** [Explanation]

---

## Overall Winner

### Winner: [Platform Name]

**Overall Score:** [X/10]

**Why This Platform Won:**
1. [Reason 1]
2. [Reason 2]
3. [Reason 3]

**Margin of Victory:** [Close / Clear / Dominant]

**Runner-up:** [Platform Name] at [Y/10]

---

## Cost-Benefit Analysis

| Platform | Cost | Quality | Speed | Value Score |
|----------|------|---------|-------|-------------|
| Platform 1 | [$X or Free] | [X/10] | [X/10] | [X/10] |
| Platform 2 | [$Y or Free] | [Y/10] | [Y/10] | [Y/10] |
| Platform 3 | [$Z or Free] | [Z/10] | [Z/10] | [Z/10] |

**Best Value:** [Platform]

**Comments:** [Your analysis of cost vs. benefit]

---

## Verdict

### Summary

[2-3 paragraph summary of your findings]

### Key Takeaways

1. [Key takeaway 1]
2. [Key takeaway 2]
3. [Key takeaway 3]

### When to Use Each Platform

**Platform 1:** [Use cases]

**Platform 2:** [Use cases]

**Platform 3:** [Use cases]

---

## Future Testing

### What to Test Next

1. [Next test suggestion]
2. [Next test suggestion]
3. [Next test suggestion]

### Improvements for Next Test

1. [Improvement 1]
2. [Improvement 2]
3. [Improvement 3]

---

## Data Summary (For Aggregation)

```csv
Prompt_ID,Platform,Model,Time_Min,LOC,Files,Compilation_%,Quality_Score,Pattern_%,SOLID_%,Docs_Score,Tests_%,Manual_Fixes
[PROMPT_ID],[Platform1],[Model1],[X],[Y],[Z],[%],[Score],[%],[%],[Score],[%],[N]
[PROMPT_ID],[Platform2],[Model2],[X],[Y],[Z],[%],[Score],[%],[%],[Score],[%],[N]
[PROMPT_ID],[Platform3],[Model3],[X],[Y],[Z],[%],[Score],[%],[%],[Score],[%],[N]
```

---

## Conclusion

**Final Verdict:** [Overall conclusion about which platform is best for this specific task]

**Confidence Level:** [High / Medium / Low]

**Recommendation:** [Your final recommendation]

---

**Report Completed By:** [Your Name]  
**Date:** [YYYY-MM-DD]  
**Version:** 1.0
